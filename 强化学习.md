# 强化学习

## 1 强化学习简介

### 1.1 基本概念

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ed619d0f50c59e4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

强化学习的基本概念即为在一开始未知结果的情况下，如何通过不同的决策进行学习。如图中的例子，一个行驶的车如何判断它要往左往右，可以通过不断的常试来得到结论，针对常试的结果的好坏，能够得到一个分数，分数越高越好。强化学习需要state（当前状态）、action（根据当前状态所作出的相应动作）和reward（根据当前状态以及东西所得到的分数），目标就是得到更高的reward。这三者以及状态更新的概率（转移因子）构成了一个马尔可夫决策的过程。

### 1.2 马尔可夫决策

要了解马尔可夫决策，首先先说明马尔可夫链的组成。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-305476874075a2c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

马尔可夫链由S、T组成，S为状态空间，是一个离散或联讯的状态集合，T为转移因子，即有t状态到t+1状态的概率。马尔可夫链中一个重要的性质就是下一个状态只与当前状态有关。这里可以看到我们与之前提到的概念还差action和reward。

![1554041176005](C:\Users\59845\AppData\Local\Temp\1554041176005.png)

如上图即时我们加入action和reward以后的结果，这就是一个马尔可夫决策过程。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cf4045c88b6aa606.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图描述部分可观察马尔可夫决策过程的结果，它可以说是构成了最基本的强化学习过程。在原有的马尔可夫决策过程中，引入了观察空间，它是由state和action决定的。另外的$\varepsilon$代表的是给定状态下观察结果的条件概率。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-a2d0474fb713aeed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

接下来探讨强化学习的目标函数，图中表示强化学期的流程，通过一个状态s在网络结构或者任意函数之后得到动作a，根据当前时刻t的s与a找到其状态转移后的t+1时刻的状态s，由此可以得到如上式子$p_{\theta}(s_1,a_1,...,s_T,a_T)$。现在我们需要通过这个概率分布找到目标函数，即整个概率最大化，最大化的函数可以写成如下的形式。我们还可以将马尔可夫决策过程转换为普通的马尔可夫链，如上如所示，我们把a和s同时看成s的话，那么问题就变成了一个马尔可夫链，那么转移概率可以为马尔可夫链的转移因子乘与决策函数的乘积。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-2913cea10de87e85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面的$\theta^*$就是有限状态的目标函数，底下为把问题转换为马尔可夫链的目标函数，是将每个时刻概率分布的期望进行求和。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cbc635f9ca7ad82f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于无限状态的目标函数可以如上图所示，我们的目标就是找到稳定状态的下期望的和，稳定状态即在状态转移很多次以后，状态在平均情况下已经不会发生改变，这时候由于期望的和接近于无限大所有会除以一个T。

### 1.3 强化学习的基本流程

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c42151f270c8f25b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为强化学习一个基本的例子，第一步为生成不同的samples，之后在模型中得到samples的“分数”，并依据此做出目标函数，最后更新策略，采用的可能还是梯度下降。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5a6882d528981c7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

以上是对目标函数的转换，我们将目标函数转换为条件概率的形式，这样就由一个求和公式，变成了一个类似连乘的公式。我们把里面这一部分叫做Q函数，在Q函数已知的情况下，我们的目标未变，即还是需要最大化目标函数，所以我们要使得$\pi(a_1|s_1)$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5c5aac85e7a4cc6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果说Q函数是状态和动作所带来的奖励之和，那么值函数代表的就是状态带来的值之和。我们可以理解为值函数就是Q函数的期望。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fbb455553691c0a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图体现的是我们如何对策略进行优化，第一个思路就是之前所提到过的尽量让能使得Q函数达到最大的action的策略达到1.第二个思路就是通过计算梯度来完成优化，当$Q^\pi(s,a)>V^\pi(s)$的时候，代表当前的action优于平均的基线，所以我们的目标就是让由state得到的action的概率尽可能大。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-19e5d35d54a06f07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图描述的是如何优化策略的过程，也可以说是如何优化模型的过程。1说的是如何通过模型计算出来的策略来尽可能使得Q函数或者目标函数最大，2说的就是梯度下降，3目前还不理解。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f6bbdcbe9b5ce1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

样本效率是评价一个模型的非正规指标，意思是是否通过小样本集即可使模型达到比较优秀的效果。我们为何使用低效率需要大样本的模型呢，那是因为可以通过并行计算等操作来节省时间，而样本效率高的算法可能本身比较复杂，并行的成本也就更高。





























