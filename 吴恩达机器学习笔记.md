# 吴恩达机器学习笔记

### 1 基本概念

1、监督学习可以理解为训练集已经告诉你要做什么（训练集存在标签），而无监督学习则没有告诉你要干嘛，需要机器自己找到训练集中的规律。例如分类是监督学习，而聚类是无监督学习。

2、回归问题=预测 

### 2 单变量线性回归

#### 2.1 代价函数（损失函数）

即只有一个输入的线性预测问题，下图为房价预测的示列，我们的目标就是训练如下的线性回归函数h(x)中的$\theta$。

![1524146940749](https://github.com/chrisli1995/photo/raw/master/markdown/1524146940749.png)

即让带进去的训练集（x，y）能很好的拟合到预测函数里面。做法是预测值与实际值的差距尽量的缩小。上列子中给了一个代价函数来表示他们之间的误差，这里的代价函数选用了平方误差代价函数，这是一个解决回归问题的常用手段。下图具体的操作。

![1524227586788](https://github.com/chrisli1995/photo/raw/master/markdown/1524227586788.png)

需要注意的是我们的预测函数是关于x的，而代价函数是关于$\theta$的函数。所以代价函数可以大致画成

![1524229823184](https://github.com/chrisli1995/photo/raw/master/markdown/1524229823184.png)

或者使用的等高线图来表示上面的3D图![1524230075600](https://github.com/chrisli1995/photo/raw/master/markdown/1524230075600.png)

可以从图中中心点代价函数值最小，即拟合训练集比较好。下面一章会讲如何找到使代价函数最小的参数$\theta$，即经典的梯度下降算法。

#### 2.2 梯度下降算法

目标就是找到代价函数（代价函数的未知数就是回归函数的参数$\theta$）为最小值时候的各个参数为多少（也不定是要用于代价函数，普通的函数求最小值也是可以的）。可以看如下的的思路。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-6d82a4da22240f08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

针对于选取的不同参数点，可能会得到不同的局部最优解。下面可以看看算法定义

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c878d65951d89238.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式为算法的一般式，中间的$\alpha$(学习率)是用来控制每次优化参数$\theta$的幅度（太小会造成参数优化的太慢，而太大可能超过最优参数较多，甚至会出现离最优参数越来越远的情况），所有的参数都是同步优化的，上图表示出了正确的更新方式。而上述的操作可以发现在求偏导的过程中，通过不断的得到正或者负数，参数$\theta$不断的向局部“最低”的靠拢。

最后，可以想到的是偏导会不断的趋近于零，这代表改变的将会越来越小，直到最后会越来越收敛，变化的幅度会越来越小。现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。

#### 2.3 线性回归算法

现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ba9b6368d0b90f14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，梯度下降就是对上式的J($\theta$)对每一个参数求偏导。求偏导的结果为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f23ef8c96c9ce961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最后我们得到的线性回归算法为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-92a834ac954b1a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 3 矩阵

基础在线性代数中的学习中都有所讲解

没有逆矩阵的方阵称为奇异矩阵（不满秩）

### 4 多变量线性回归

即拥有多个输入参数的线性预测问题。大致可以把之前的预测房价问题形式化为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d010576f28a78df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

输入参数变多了以后，线性回归的函数也得有相应的改变为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c3c7343d666d3372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到h($\theta$)函数有n+1个输入（x0=1没有写出来），有n+1的参数$\theta$，我们可以通过两个向量（输入向量$\theta$和参数向量$\theta$）的相乘来表示出我们的回归函数。

#### 4.1 特征缩放与学习率

本章的内容是为了更好的实现梯度下降算法。

我们的代价函数并没有太多的改变，可以定义为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e07d628f090952b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

底下即为梯度下降算法，我们多个参数$\theta$(i)以向量$\theta$的形式呈现。那么我们现在做的就是对这个代价函数求偏导。它的结果和之前的单变量线性回归的结果比较可以如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ea3420026c31e911.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这里面的过程中，可能存在输入的取值范围差别很大的情况，这个时候可以采用特征缩放的方式先将参数做一下处理。这样做了以后得到的梯度下降算法就会变得更为的收敛，可以查看下图。

![1524752280142](C:\Users\59845\AppData\Local\Temp\1524752280142.png)

上图x1和x2之前的取值范围分别为0~2000和0~5差别很大，得到的等高线图很“细窄”，这样会不利于找到最优化的参数，而像右图那样对输入进行特征缩放以后的效果就要好很多了。正常的情况是把输入x控制到-1~1之间，可以通过减去类似某个输入的平均值的方法达到正负的效果（归一化）。

如果代价函数在梯度下降的不断迭代下呈现不断上升的趋势（或者出现波动），这时候就需要对学习率相对的减小一点。

当然对于整个回归函数的输入，你可以选择不同特征作为输入，比方说x的二次方，三次方，这样得到的回归函数能够更好的拟合数据。

#### 4.2 正规方程

与梯度下降算法一样，正规方程也是用于求代价函数中最优的参数$\theta$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-1d32c09e6c2c5f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从上式看到，其实就是原来学过的如何求最低点的问题扩展到了多维向量中。而具体的实现步骤如下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-7d66d26210c2c1c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么什么时候使用梯度下降，什么时候使用正规方程呢？对于大量数据的数据集，采用梯度下降算法的性能会更好一些。通常数据集过万后就讨论采用梯度下降了。

### 5 Logistic回归

单纯的线性回归对分类的问题的拟合不是很好，比如我的输出是0或1（对应的二分类问题），但采用线性回归得到的预测值远大于或远小于这个数，这个时候就可以采用logistic回归，它可以把所有的输出都控制在0到1之间。所以logistic回归实际是一种分类算法。

#### 5.1 Sigmoid函数

可以和logistics回归理解为同一个概念，它把所有的输入都映射到了0到1之间，如下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-030c05fcec4d5f70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

x即为我们的输入，$\theta$为我们的优化参数，它很好的规定了我们的输出。一般的情况下，是把这个函数比作通过输入的特征得到某个分类结果的可能性，可以形式化的表示为h(x)=p(y=1|x;$\theta$)

![image.png](https://upload-images.jianshu.io/upload_images/7810235-d73f95017d2d4773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当输入后的预测大于等于0.5的时候，我们认定输出为1，反之则为0，即输入要大于0，$\theta$的转置乘与输入x要大于0。

当我们的每一个参数$\theta$都确定以后，带入我们回归函数，就能确定一条“线”（二维的情况下），这条线叫做决策边界。决策边界不是有输入的数据集决定的，而是在$\theta$确定后生成的。

若跟之前一样采用的线性回归的代价函数，则会出现非凸函数，有许多局部最优解，梯度下降算法不怎么好使用。这里就要用到一个新的代价函数表示了（Word2vec就是用的这种）

![image.png](https://upload-images.jianshu.io/upload_images/7810235-44a0fad4256d7b12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当结果为1的时候，h(x)=1的时候代价函数为0，这样很好的说明我预测正确以后，代价是最小的，而h(x)趋近于0的时候，当结果为0的时候，有同样的推导。我们可以进一步将上式进行合并，写为Cost(h$\theta$(x),y)= -y*log(h$\theta$(x)) - (1-y)log(1-h$\theta$(x))。现在我们有了代价函数，目标就是求出使得代价函数最小的$\theta$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3e0957b227e112de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个地方跟word2vec中的表示一样，计算步骤可以看那一块的知识总结。 

#### 5.2 扩展

还有许多类似梯度下降的优化算法，比如Conjugate gradirnt、BFGS、L-BFGS等，它们的优势是能够自动选取学习率，而且收敛的速度远快于梯度下降。

上述均为处理二分类的方法，那么遇到多分类的时候如何处理？做法是只识别当前要分类项和非该分类项，即把多分类问题当做多个而分类问题处理。![image.png](https://upload-images.jianshu.io/upload_images/7810235-d81a75702d38adf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 6 正则化

#### 6.1 拟合问题

在函数适应数据集的过程中可能出现不能很好适应训练集的情况（欠拟合）或者虽然对所有训练集的数据有很好的拟合，不适应测试集的情况（过拟合）。出现过拟合的常见原因是因为特征选的过多，而训练集的量过小。

那么如何解决过拟合的问题呢，最简单的就是把训练好的函数画出来，观察它是否过拟合，显然这种方法不常用。常见的方法是减少特征和正则化。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-67ae9d498aff9577.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 6.2 正则化

