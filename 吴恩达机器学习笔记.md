# 吴恩达机器学习笔记

### 基本概念

1、监督学习可以理解为训练集已经告诉你要做什么（训练集存在标签），而无监督学习则没有告诉你要干嘛，需要机器自己找到训练集中的规律。例如分类是监督学习，而聚类是无监督学习。

2、回归问题=预测 

### 单变量线性回归

#### 代价函数（损失函数）

即只有一个输入的线性预测问题，下图为房价预测的示列，我们的目标就是训练如下的线性回归函数h(x)中的$\theta$。

![1524146940749](https://github.com/chrisli1995/photo/raw/master/markdown/1524146940749.png)

即让带进去的训练集（x，y）能很好的拟合到预测函数里面。做法是预测值与实际值的差距尽量的缩小。上列子中给了一个代价函数来表示他们之间的误差，这里的代价函数选用了平方误差代价函数，这是一个解决回归问题的常用手段。下图具体的操作。

![1524227586788](https://github.com/chrisli1995/photo/raw/master/markdown/1524227586788.png)

需要注意的是我们的预测函数是关于x的，而代价函数是关于$\theta$的函数。所以代价函数可以大致画成

![1524229823184](https://github.com/chrisli1995/photo/raw/master/markdown/1524229823184.png)

或者使用的等高线图来表示上面的3D图![1524230075600](https://github.com/chrisli1995/photo/raw/master/markdown/1524230075600.png)

可以从图中中心点代价函数值最小，即拟合训练集比较好。下面一章会讲如何找到使代价函数最小的参数$\theta$，即经典的梯度下降算法。

#### 梯度下降算法

目标就是找到代价函数为最小值时候的各个参数为多少（也不定是要用于代价函数，普通的函数求最小值也是可以的）。可以看如下的的思路。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-6d82a4da22240f08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

针对于选取的不同参数点，可能会得到不同的局部最优解。下面可以看看算法定义

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c878d65951d89238.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式为算法的一般式，中间的$\alpha$(学习率)是用来控制每次优化参数$\theta$的幅度（太小会造成参数优化的太慢，而太大可能超过最优参数较多，甚至会出现离最优参数越来越远的情况），所有的参数都是同步优化的，上图表示出了正确的更新方式。而上述的操作可以发现在求偏导的过程中，通过不断的得到正或者负数，参数$\theta$不断的向局部“最低”的靠拢。

最后，可以想到的是偏导会不断的趋近于零，这代表改变的将会越来越小，直到最后会越来越收敛，变化的幅度会越来越小。现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。

#### 线性回归算法

现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ba9b6368d0b90f14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，梯度下降就是对上式的J($\theta$)对每一个参数求偏导。求偏导的结果为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f23ef8c96c9ce961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最后我们得到的线性回归算法为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-92a834ac954b1a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 矩阵

基础在线性代数中的学习中都有所讲解

没有逆矩阵的方阵称为奇异矩阵（不满秩）

### 多变量线性回归

即拥有多个输入参数的线性预测问题。大致可以把之前的预测房价问题形式化为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d010576f28a78df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

输入参数变多了以后，线性回归的函数也得有相应的改变为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c3c7343d666d3372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到h($\theta$)函数有n+1个输入（x0=1没有写出来），有n+1的参数$\theta$，我们可以通过两个向量（输入向量$\theta$和参数向量$\theta$）的相乘来表示出我们的回归函数。

#### 特征缩放

本章的内容是为了更好的实现梯度下降算法。

我们的代价函数并没有太多的改变，可以定义为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e07d628f090952b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

底下即为梯度下降算法，我们多个参数$\theta$以向量$\theta$的形式呈现。那么我们现在做的就是对这个代价函数求偏导。它的结果和之前的单变量线性回归的结果比较可以如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ea3420026c31e911.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这里面的过程中，可能存在输入的取值范围差别很大的情况，这个时候可以采用特征缩放的方式先将参数做一下处理。这样做了以后得到的梯度下降算法就会变得更为的收敛，可以查看下图。

![1524752280142](C:\Users\59845\AppData\Local\Temp\1524752280142.png)

上图x1和x2之前的取值范围分别为0~2000和0~5差别很大，得到的等高线图很“细窄”，这样会不利于找到最优化的参数，而像右图那样对输入进行特征缩放以后的效果就要好很多了。正常的情况是把输入x控制到-1~1之间，可以通过减去类似某个输入的平均值的方法达到正负的效果（归一化）。

