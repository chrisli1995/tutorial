吴恩达机器学习笔记

### 1 基本概念

1、监督学习可以理解为训练集已经告诉你要做什么（训练集存在标签），而无监督学习则没有告诉你要干嘛，需要机器自己找到训练集中的规律。例如分类是监督学习，而聚类是无监督学习。

2、回归问题=预测 

### 2 单变量线性回归

#### 2.1 代价函数（损失函数）

即只有一个输入的线性预测问题，下图为房价预测的示列，我们的目标就是训练如下的线性回归函数h(x)中的$\theta$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-7922c4f2e4ec0426.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

即让带进去的训练集（x，y）能很好的拟合到预测函数里面。做法是预测值与实际值的差距尽量的缩小。上列子中给了一个代价函数来表示他们之间的误差，这里的代价函数选用了平方误差代价函数，这是一个解决回归问题的常用手段。下图具体的操作。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3d9ba39f7b11432b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

需要注意的是我们的预测函数是关于x的，而代价函数是关于$\theta$的函数。所以代价函数可以大致画成

![image.png](https://upload-images.jianshu.io/upload_images/7810235-62ed279701e057bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

或者使用的等高线图来表示上面的3D图。![image.png](https://upload-images.jianshu.io/upload_images/7810235-0c006461557cf16f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



可以从图中中心点代价函数值最小，即拟合训练集比较好。下面一章会讲如何找到使代价函数最小的参数$\theta$，即经典的梯度下降算法。

#### 2.2 梯度下降算法

目标就是找到代价函数（代价函数的未知数就是回归函数的参数$\theta$）为最小值时候的各个参数为多少（也不定是要用于代价函数，普通的函数求最小值也是可以的）。可以看如下的的思路。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-6d82a4da22240f08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

针对于选取的不同参数点，可能会得到不同的局部最优解。下面可以看看算法定义

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c878d65951d89238.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式为算法的一般式，中间的$\alpha$(学习率)是用来控制每次优化参数$\theta$的幅度（太小会造成参数优化的太慢，而太大可能超过最优参数较多，甚至会出现离最优参数越来越远的情况），所有的参数都是同步优化的，上图表示出了正确的更新方式。而上述的操作可以发现在求偏导的过程中，通过不断的得到正或者负数，参数$\theta$不断的向局部“最低”的靠拢。

最后，可以想到的是偏导会不断的趋近于零，这代表改变的将会越来越小，直到最后会越来越收敛，变化的幅度会越来越小。现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。

#### 2.3 线性回归算法

现在将梯度下降算法代入上节中的代价函数中得到了线性回归算法。如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ba9b6368d0b90f14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，梯度下降就是对上式的J($\theta$)对每一个参数求偏导。求偏导的结果为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f23ef8c96c9ce961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最后我们得到的线性回归算法为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-92a834ac954b1a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 3 矩阵

基础在线性代数中的学习中都有所讲解

没有逆矩阵的方阵称为奇异矩阵（不满秩）

### 4 多变量线性回归

即拥有多个输入参数的线性预测问题。大致可以把之前的预测房价问题形式化为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d010576f28a78df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

输入参数变多了以后，线性回归的函数也得有相应的改变为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c3c7343d666d3372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到h($\theta$)函数有n+1个输入（x0=1没有写出来），有n+1的参数$\theta$，我们可以通过两个向量（输入向量$\theta$和参数向量$\theta$）的相乘来表示出我们的回归函数。

#### 4.1 特征缩放与学习率

本章的内容是为了更好的实现梯度下降算法。

我们的代价函数并没有太多的改变，可以定义为

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e07d628f090952b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

底下即为梯度下降算法，我们多个参数$\theta$(i)以向量$\theta$的形式呈现。那么我们现在做的就是对这个代价函数求偏导。它的结果和之前的单变量线性回归的结果比较可以如图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ea3420026c31e911.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在这里面的过程中，可能存在输入的取值范围差别很大的情况，这个时候可以采用特征缩放的方式先将参数做一下处理。这样做了以后得到的梯度下降算法就会变得更为的收敛，可以查看下图。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-2c267a40e05ed4a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图x1和x2之前的取值范围分别为0~2000和0~5差别很大，得到的等高线图很“细窄”，这样会不利于找到最优化的参数，而像右图那样对输入进行特征缩放以后的效果就要好很多了。正常的情况是把输入x控制到-1~1之间，可以通过减去类似某个输入的平均值的方法达到正负的效果（归一化）。

如果代价函数在梯度下降的不断迭代下呈现不断上升的趋势（或者出现波动），这时候就需要对学习率相对的减小一点。

当然对于整个回归函数的输入，你可以选择不同特征作为输入，比方说x的二次方，三次方，这样得到的回归函数能够更好的拟合数据。

#### 4.2 正规方程

与梯度下降算法一样，正规方程也是用于求代价函数中最优的参数$\theta$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-1d32c09e6c2c5f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从上式看到，其实就是原来学过的如何求最低点的问题扩展到了多维向量中。而具体的实现步骤如下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-7d66d26210c2c1c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么什么时候使用梯度下降，什么时候使用正规方程呢？对于大量数据的数据集，采用梯度下降算法的性能会更好一些。通常数据集过万后就讨论采用梯度下降了。

### 5 Logistic回归

单纯的线性回归对分类的问题的拟合不是很好，比如我的输出是0或1（对应的二分类问题），但采用线性回归得到的预测值远大于或远小于这个数，这个时候就可以采用logistic回归，它可以把所有的输出都控制在0到1之间。所以logistic回归实际是一种分类算法（二分类）。

#### 5.1 Sigmoid函数

可以和logistics回归理解为同一个概念，它把所有的输入都映射到了0到1之间，如下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-030c05fcec4d5f70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

x即为我们的输入，$\theta$为我们的优化参数，它很好的规定了我们的输出。一般的情况下，是把这个函数比作通过输入的特征得到某个分类结果的可能性，可以形式化的表示为h(x)=p(y=1|x;$\theta$)

![image.png](https://upload-images.jianshu.io/upload_images/7810235-d73f95017d2d4773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当输入后的预测大于等于0.5的时候，我们认定输出为1，反之则为0，即输入要大于0，$\theta$的转置乘与输入x要大于0。

当我们的每一个参数$\theta$都确定以后，带入我们回归函数，就能确定一条“线”（二维的情况下），这条线叫做决策边界。决策边界不是有输入的数据集决定的，而是在$\theta$确定后生成的。

若跟之前一样采用的线性回归的代价函数，则会出现非凸函数，有许多局部最优解，梯度下降算法不怎么好使用。这里就要用到一个新的代价函数表示了（Word2vec就是用的这种）

![image.png](https://upload-images.jianshu.io/upload_images/7810235-44a0fad4256d7b12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当结果为1的时候，h(x)=1的时候代价函数为0，这样很好的说明我预测正确以后，代价是最小的，而h(x)趋近于0的时候，当结果为0的时候，有同样的推导。我们可以进一步将上式进行合并，写为Cost(h$\theta$(x),y)= -y*log(h$\theta$(x)) - (1-y)log(1-h$\theta$(x))。现在我们有了代价函数，目标就是求出使得代价函数最小的$\theta$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3e0957b227e112de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个地方跟word2vec中的表示一样，计算步骤可以看那一块的知识总结。 

#### 5.2 扩展

还有许多类似梯度下降的优化算法，比如Conjugate gradirnt、BFGS、L-BFGS等，它们的优势是能够自动选取学习率，而且收敛的速度远快于梯度下降。

上述均为处理二分类的方法，那么遇到多分类的时候如何处理？做法是只识别当前要分类项和非该分类项，即把多分类问题当做多个而分类问题处理。![image.png](https://upload-images.jianshu.io/upload_images/7810235-d81a75702d38adf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 6 正则化

#### 6.1 拟合问题

在函数适应数据集的过程中可能出现不能很好适应训练集的情况（欠拟合）或者虽然对所有训练集的数据有很好的拟合，不适应测试集的情况（过拟合）。出现过拟合的常见原因是因为特征选的过多，而训练集的量过小。

那么如何解决过拟合的问题呢，最简单的就是把训练好的函数画出来，观察它是否过拟合，显然这种方法不常用。常见的方法是减少特征和正则化。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-67ae9d498aff9577.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 6.2 正则化

正则化的思想可以简单的理解为尽量使一些参数较小才能达到代价函数最小化，做法是在参数前面加一个“惩罚项”，即加入一个较大值，只有当参数很小的时候才能使得代价函数达到最小。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e45c784455aa2491.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是，我们不知道哪些参数是相关度比较低，所以只能对所有的参数进行正则化处理，处理的方式如果上面的例子那样引入一个“惩罚项”。

##### 6.2.1 线性回归正则化

下面我们可以看看之前在线性回归中所学的代价函数引入正则化以后的式子。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f9fadd5fbddd2731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中的$\lambda$为正则化参数，$\lambda$设置的过大的话，会使得参数$\theta$过小，这样整个回归函数会变成一条类似的直线h$\theta$(x)=$\theta$0（式子写的不好。。。），而参数过小的话，又起不到实际的意义。对新的代价函数J($\theta$)求偏导以后就可以得到新的梯度下降算法。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f1e3d805bde06b95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式可以看到就是对于$\theta$的更新，将2式中的$\theta$j提出来合并同类项可以得到3式，3式中的（1-$\alpha*$$\lambda$/m）在一般情况下是一个接近于1的数，而3式后面的一部分与之前的梯度下降是一样的。 而在正规方正中加入正则化的形式如下：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5a3e184ad61eefaf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##### 6.2.2 Logistic回归正则化

对于logistic回归的问题，方法同样是添加“惩罚项”来保持参数较小。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d55cb20a10811f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为对logistic回归的代价函数加入正则化的式子，而上式的梯度下降式与之前一样。 

### 7 神经网络

存在的原因是因为上述所说的一些机器学习算法，在特征过多，并且存在多次项这种非线性情况的时候，计算量较大。（线性函数即为只包含一次项的式子，非线性则会包含多次项，比如X1*X2或者$X1^2$）

#### 7.1 常识

基本的理解就是神经网络通过模拟大脑神经元的工作方式，以计算机和算法来得到类似于大脑处理图像、文字等信息的方法。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c54d6f486ccfe14c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个就是简单的神经网络表示形式，x即为输入，a即为中间神经元，而右边的黄圈为输出，一般是还要对输出加入一个h(x)的激活函数，通过激活函数，可以将线性的函数改成非线性的函数，从而达到更好的效果，因为如果是一个线性的模型，往往是解决不了实际的问题的。

而其中的有三层的结构：

layer1：为输入层，是将特征输入

layer2：为隐藏层，用于训练参数

layer3：为输出层

具体的计算步骤可以看下图：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e04189a8728053b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

$a{_i^j}$表示的就是由上一层的输入经过参数运算（在上图中用的SIGMOD函数）以后的结果（可以叫做中间神经元或者单元），而$\Theta^j$表示的是控制j到j+1层的输出的参数。可以从上图中看出，整个$\Theta^j$的形式为一个3*4的矩阵，矩阵的大小一般化如上图下面的理解形式，即大小为（j+1层的单元数）（j层的单元数+1），关于为什么要加1是因为要加入一个额外的偏执神经元，即图中的$x_0$=1，具体的操作可以从如下图中看到。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c204902908821f75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面的过程是更为具体的改模型的计算步骤，之中引入了$z$的概念来方便理解。这种一步一步从第j层到第j+1的操作，叫做**前向传播**。如果单看$h_\Theta(x)$的话（图片黄色神经元部分），可以看到它与logistic回归的函数比较像。

这里只有三层神经网络结构，也可以在中间加入更多的网络结构。前一层的输出即为后一层的输入，而每一层的神经元个数也可以是任意的，下面我们看一个例子是关于如何构建一个函数表示$x_1$XNOR$x_2$的（XNOR表示同或，即同为0或1，才输出1）。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fe716c1220f83afe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 这图好像没什么好讲的，主要是便于理解神经网络的工作机制。需要注意的是输出不一定只是单个神经元，可能是多个神经元的组合。

#### 7.2 反向传播算法

##### 7.3.1 基本计算过程

神经网络的代价函数可以和之前的logistic回归函数的代价函数做类比

![image.png](https://upload-images.jianshu.io/upload_images/7810235-170cc916e62978a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

一般正则化的那一项不会添加，K所说的是输出神经元的个数，m为输入的个数，相较于logistic回归只有一个输出，神经网络是可能存在多个输出的，所以最后的输出$h\Theta(x)$是为一个K维的向量，L为神经网络的总层数。

得到了代价函数以后，我一般想到的就是通过梯度下降对他进行求解最优参数。下面通过两张图来理解一下什么是反向传播算法。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-d60c255182d58354.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](https://upload-images.jianshu.io/upload_images/7810235-db0b7b2d070a4455.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

反向传播算法的本质是用于求梯度下降中的偏导数，$\delta_j^{(l)}$的意思是第l层网络第j个节点的误差，图2中有一个简单的神经网络的示列，可以看到除了第一层（输入层）以外，都有一个误差项。通过上图的下式即可得到对某一个中间参数的偏导。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9e64f2d37977abad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为反向传播算法的一个具体的思路，有了如何求得中间参数的误差项，然后对他进行累加，之后的$D_{ij}^{(l)}$即为求得中间参数的偏导项。

##### 7.3.2 理解 

上面有些式子解释的不够清楚，所以进行进一步的解释。其实从本质上讲，反向传播算法的目的就是在神经网络已经得到代价函数以后，通过反向传播算法对每一个中间神经元（中间的参数）求关于代价函数的偏导数，以此用于梯度下降算法，求得最优参数。这看似还是有点难理解，最简单一种理解是反向传播其实就是正向传播反过来看，传入下一层的参数即之前求解的$\delta_j^{(l)}$。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-c4fe8838feef6849.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图有清楚的过程来看到$\delta$的值是如何求得，方式跟正向传播类似，需要注意的是反向传播一般不考虑偏置项，因为对结果没有影响。

#### 7.3 梯度检验

用于检验梯度下降算法中（可能还带有反向传播算法）中求得的偏导数是否正确，具体的方式类似于我们原来高数求偏导的形式，如图。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e9defc2cdb109029.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图所得的结果与实际梯度下降得到的结果对比，如果误差较小则可以大致说明计算的梯度没有问题，需要主要在检验完成后，将梯度检验算法关闭，减小代码的时间。

### 8 卷积神经网络（CNN）

#### 8.1 基础概念

##### 8.1.1 卷积层

相当于二维的映射层，也可以叫做3D滤波器。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3ff9973e3e54893d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由图中可以看出对于每一层卷积层由多个卷积核构成，一个卷积核会形成一层特征图，卷积核用于提取局部特征。具体的步骤可以看下图：

 ![image.png](https://upload-images.jianshu.io/upload_images/7810235-211e02e3eaa48976.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图中WHD分别是关于特征图的尺寸和个数，可以看到不同的卷积核可以通过零填充来得到规定大小的特征图，可以理解为长宽高。如果是一个三通道的图，可以理解为三“片”单通道的图通过卷积核得到特征图以后，再对三个特征图进行一个相加的整合。图中的这些都是超参数。下面可以看一个具体的例子，说明了卷积核是如果工作的。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-75a818eadff83916.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图可知，输入的是一个三通道的（RGB）的彩色图片，尺寸是32X32的，即一个输入的图片为32X32X3的张量，而卷积核的大小为5X5X3（此处的3是根据输入的通道数来决定的），然后卷积核对图片进行扫描的操作，得到一个28X28X1的特征图（这里的操作可以参照上一张图片），因为上图有六个卷积核，所以有六张特征图。最后结果就是一个输入为32X32X3的图片通过6个卷积核变成了一个28X28X6的特征图。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-d878e0f68da5f846.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这是所使用的ReLU激活函数，不是采用原来的SIGMOD函数（SIGMOD函数在两极的时候容易出现梯度消失的问题），可以看到他将数据都规定到“第一象限”。有了卷积核和激活函数即可完成卷积的操作。可以由如下图来理解问题。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cbcfba24a375c6f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图可视，两个通道的输入通过两个卷积核得到一个两层的输出，然后再通过激活函数计算后，再通过一个卷积核对特征图进行降维的操作，最后再通过激活函数得到降维后的结果。

##### 8.1.2 池化层

用于特征降维，通常有平均池化和最大化池化。用于特征融合和降维 。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-78fb3a3899096263.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

具体的用法是将特征图按规定进行扫描，最后将扫描区域内的数值的最大值或者平均值，需要注意的是与卷积核不同，池化的时候不会重复的扫描中间的区域。池化层没有需要学习的参数，都是事先定义好的超参。

##### 8.1.3 全连接层

即传统神经网络的映射层![image.png](https://upload-images.jianshu.io/upload_images/7810235-494e0bb211f46c20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图的抽象化理解不是很清楚，以自己的理解来说，就是当卷积层得到的一些参数“平铺”为一个向量，之后就与传统神经网络的方式相同，通过中间神经元得到下一层的参数。

##### 8.1.4 CNN-Softmax层

![1531572767610](C:\Users\59845\AppData\Local\Temp\1531572767610.png)

这一层就是决定最后分类结果的最后一层了，将最后一个全连接层得到的结果作为输入带到里面以后，得到输出层的每一个神经元（可以理解为标签）的结果了，结果最大的值即为预测的标签，与传统的神经网络一样需要用损失函数进行计算。

### 9 自然语言处理（NLP）

 





