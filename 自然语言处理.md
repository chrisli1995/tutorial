# 自然语言处理

笔记中包含了NLP的学习以及所设计到语义关系的一些技术，比如社交网络等。**默认有一定的基础**

视频连接或百度云：

https://www.bilibili.com/video/av23496260/（主讲RNN和seq2seq）

链接: https://pan.baidu.com/s/1NBn-y5-mWgE3xjTyM3YEYQ 密码: b9qb（主讲word2vec）

## 1 概论

### 1.1 什么是自然语言处理（NLP）

使用计算机处理文本与声音

本质上说的是将文本进行转换，可以理解为：

-  **文本->表示**：比如情感分析就是将句子转换为-1/0/+1等不同的值，而句法分析则是将句子转换为语法树（AST）
-  **表示->文本**：比如可以通过主题词生成诗歌
-  **文本->文本**：比如机器翻译


### 1.2 常见应用

应用范围非常广泛，就比如在平时用的输入法上也有很多的利用：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-315dbc673bade36c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如在机器翻译中的应用：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-408f8938caf4e739.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如搜索引擎中的自动化处理：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-36122d4c00c61483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如我们看文章中时候出现的推荐功能：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0dce5221619b3851.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如我们之前也见过的聊天机器人：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-b8936bf6efbf8792.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如英文写作助手，这个其实不怎么常见：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-aa6cfa3729a6c71a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当讲完这些基本的应用背景以后，也有一些比较新颖的应用背景，比如：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-8108855b103c4d5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这一类设计的技术可能就不仅仅包括NLP方面，可以看到通过Twitter检测重要事事故，同样需要用到社交网络等知识。也有与图像识别一起使用的一些应用点，比如：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cb497517b5c643d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如，可能运用的方面比较新颖，技术方面不会太过复杂：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-20b4a15a090d51f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如一些谷歌的自动回复功能：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0abdb25b6c4a1ff2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于上面这些应用，我们所涉及到的技术可以总结为如下图：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4de148edf5092229.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

社交网络和可信度分析不完全属于NLP的范畴，但有用到。其他的是需要了解的技术。后面会有讲解。

### 1.3 挑战

语言的挑战主要还是如何准确的判断出每个词在语句中的真实含义，比如需要看上下文的语境。对于自然语言处理，他处理的层次如图。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-220e7ab9ffcb49ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

大致总结为我们需要使用词法和语法来分析语义和语用，语用的概念为句与句之间的关系。

其次，对于领域隔离也是一个难点所在，不同领域的不同情况，机器的理解还是完全跟不上的，跨过任何一个领域都比较困难，一种新颖的观点是语言自己就是最好的表示。

再然后就是数据集的搜索难度较大，比如少数民族的语言或者被和谐的语言。

最后很多情况下无法评判结果的好坏，比如机器翻译，一句话有多种意思都是对的，不能说哪个翻译是对的哪个是错的。  

有一些常用的工具如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f411d818dee8c1e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2 语言模型

**语言模型**抽象的理解就是为说的每一句话做一个评分，可以形式化的理解为score(文本)-->评分（score）。以此还可以扩展出**话题模型**，其形式为score（话题|文本）-->评分（score），比如可以定义score（NLP|什么是语言模型）-->0.8，而score（三国|什么是语言模型）-->0.05，这种模式就是话题模型。

而在NLP中的语言模型，所说的是概率语言模型。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9e9d89d55e1a2468.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，式子反应一句话出现的概率，所有话出现的概率总和为1。

### 2.1 N-gram模型

目的是求一句话出现的概率，采用了链式规则，其实比较简单，大致的思想就是

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9e04af9049a7339f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是这样计算的参数比较多，所以引入了马尔科夫假设，即无记忆性（未来的事件，只取决于有限的历史）的概念。 

![image.png](https://upload-images.jianshu.io/upload_images/7810235-59f080efb8941307.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

N-gram模型设定的是一个词出现的概率，之前其之前有限个词出现的概率有关，这样就大大的减小了计算的难度，但RNN（循环神经网络）很好的解决了计算难度大的问题，所以又可以直接采用链式规则求出一句话出现的概率。常见的N-gram模型有：

**Bi-Gram**:P(T)=p(w1|begin)*p(w2|w1)*p(w3|w2)***p(wn|wn-1)

**Tri-Gram**:P(T)=p(w1|begin1,begin2)*p(w2|w1,begin1)*p(w3|w2w1)***p(wn|wn-1,wn-2)

### 2.2 语言模型的评价标准

评标标准在任何模型中都是非常的重要，如何较好的评价出模型的评分或者预测的准确率至关重要，语言模型中大致还是使用预测测试集的准确程度来评价。具体的步骤可以如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-8a636412f181adde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们的目的在于将得到的perplexity的值最小化，**$q(w_i)$的意思是，模型计算出来的$w_i$在前后关系中出现的概率**。那么perplexity到底是什么意思呢，其实可以表示为测试集的等效状态的数目，再通俗一点可以理解为我需要通过多少词才能预测准确测试集。他的具体数学逻辑可以这么理解：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4c719e4355330c05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中的N代表的是测试集中所包含的单词数，V则代表测试集中不算重复的单词数，经过转换可以得道最后的公式，其中$\hat{p}(v_i)$的意思是$v_i$在测试集中出现的概率。最简单的思想就是我们的目标是最大化这里面出现的正确词的概率，preplexity表明了你预测正确词需要在其等效的数目中去选取，比如preplexity等于64的话，正确词选取的概率就是$\frac{1}{64}$，即$q(w_i)$增大的时候perplexity是减小的，可能这里还是不是很理解，但至少要理解上述所说的话。（可以去学习一下熵的概念，如果有人告诉我们一个相当不可能的时间发生了，我们收到的信息要多于我们被告知某个很可能发生的事件发生时收到的信息，如果我们知道某件事情一定会发生,那么我们就不会接收到信息。于是，我们对于信息内容的度量将依赖于概率分布 p(x) ，因此我们想要寻找一个函数 h(x) ，它是概率 p(x) 的单调递减函数，表达了信息的内容。）

### 2.3 N-gram模型遇到的难点

这里我们引入一个新的概念，OOV（out of vocabulary），顾名思义就是说没有的单词，即我们的模型如何预测训练集中没有的模型，这是语言模型中的一个难点。另外，当实际情况中，出现了没有出现的单词组合我们怎么办，这也是一个难点。下面我们可以举个例子来说明下：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-36566c5a9b356a06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当遇到OOV的情况的时候，我们改如何处理呢，$P(w_i|w_{i-1},w_{i-2})$的意思是当出现$w_{i-1}$和$w_{i-2}$的时候，下一词是$w_i$的可能性。对于第一种情况，测试集出现了训练集不存在的词，比如王者农药，做法是将训练集中出现词频最小的词改为“UNK”,这样王者农药就属于这里的UNK。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d1e9fa97c838bad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到修改后，出现王者农药的概率由原来的0到现在的$\frac{1}{3}$。对于解决我们没有出现的序列，其实有很多的方法，大致可以分为下图中的三类。![image.png](https://upload-images.jianshu.io/upload_images/7810235-b7f48b4561148e9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**+1平滑**相当于在概率的基础上加上一个值，使得概率不会变为0，但语言模型一般不使用。

**Back-off回退法**说的是当没出现出现的序列的情况下，我们可以简化序列，比如将Trigram的情况会出现有的概率为0的情况，则使用Brigram，如果还有概率为0的情况可以使用Unigram。

**Interpolate插值法**所用的方法是将Trigram、Brigram、Unigram一定比例混合使用，这样的话即使Trigram出现没有的序列而导致概率为0，其他几种情况的概率依旧可以使得最后的概率不为0,参数的选择是通过训练集得出，并且可以依据不同的词来调节参数。

**Absolute Discount**字面上理解就是绝对折扣的意思，大致的思路就是设置一个阈值D，当没达到这个阈值D的时候，改为使用Unigram。 ![image.png](https://upload-images.jianshu.io/upload_images/7810235-f384c5d1d372d349.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)可以更抽象的理解是，当一个赚的钱不足D的时候，我只能去依靠向别人借钱来维持，这里的别人就是后面的$P_{abs}=(w_i|w^{i-1}_{i-n+2})$,即Brigram。

**Kneser-Ney Smoothing**这种方法是对**Absolute Discount**的改进，对于**Absolute Discount**可能会出现在使用Unigram的时候频率比价高但与实际效果并不符合，例如（刘强东 奶茶）这个组合出现的比（王思聪 张予馨）和（王思聪 豆得儿）等一些词加起来还多，但当你要预测P（*，网红）的时候，并不希望出现的是刘强东的概率更高，所以我们希望“交际广泛”的王思聪出现的更多。![image.png](https://upload-images.jianshu.io/upload_images/7810235-974bbcac5b4f75c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)上图中就说明主要改进的是$P_{KN}$，$N_{1+}(x)$的概念是所有出现的词组的**个数**（不是次数）。 

**Modified Kneser-Ney Smoothing**则是对**Kneser-Ney Smoothing**又一改进，设置了不同的D值。![image.png](https://upload-images.jianshu.io/upload_images/7810235-454db205c3396495.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)这个地方的理解还不是很深刻。。。

下面是对所有方法的一个总结：![image.png](https://upload-images.jianshu.io/upload_images/7810235-fdb61ef7bc789efa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.4 代码实现

对于环境可以是anaconda的科学python库，采用nltk包对英文文本进行分词。主要的示列存在GitHub上，地址为：https://github.com/chrisli1995/xing_nlp。

## 3 word2vec原理

### 3.1 为什么是word2vec？

在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder（独热码）。 如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0b27b582c6d5c8fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot  Encoder有以下问题。首先，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 

**Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。** 

它的主要用途可以分为：

- 寻找近义词
- 用作别的NLP任务的特征值
- 用来为别的neural network作初始化词向量

### 3.2 基本知识

#### 3.2.1 哈夫曼树	

#### 3.2.2 逻辑回归

### 3.3 CBOW和skip-gram



## 4 社会网络

### 4.1 基本概念

社会网络中，个体之间的互动和交流，包括信息的推荐分享等，使得信息在网络中得到扩散。社会网络的结构可用**有向图**G=(V,E)表示，其中节点集合V中的元素代表社会网络中的社会个体，边集E中的元素代表社会个体间的关系。有向图中每个节点初始时均处于“未激活”状态，为“未激活节点”，如果网络中的某个个体接受某个观点，或购买了某个产品，则称该个体对应的节点处于“激活状态”，转变为“激活节点”。

下面会介绍一些社会网络的基本概念：

**激活**： 节点u 处于激活状态时，会对其邻居节点中处于未激活状态的节点v 产生影响， 当节点v 受节点u 的影响转变为激活节点时， 称节点u将节点v “激活” 。 

**被激活**： 根据“激活”的描述， 节点v 受节点u 的影响作用“被激活”。 

**扩散过程**： 指网络中未激活节点受激活节点的影响， 转变成激活状态的过程， 即下图中，由t时刻向t1时刻转变的过程。其中，浅色节点为激活节点（如节点2）， 深色节点为未激活节点（如节点 10）。t到t1时刻，节点1受节点2和4的共同影响作用被激活。 在扩散过程中，节点只能由未激活状态转变成激活状态，且当节点 1 被激活后，它将一直保持激活状态，而且会再去激活它自己的未激活邻节点7和9。重复这一过程，直至网络中所有的节点都被激活，或节点状态均不再发生变化为止。 

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4d08e9075b3732d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**影响力**： 社会群体中，当社会个体表现出某个行为后，可能会改变群体中其他个体的思想、情感或行为，这种影响其他个体的能力称为该个体在该群体中的影响力。 

### 4.2 影响力扩散问题

#### 4.2.1 MAXINF

即影响最大化问题，探索一些初始的有影响力的节点，使得最后从节点扩散开的节点达到最大化。

问题的形式化描述为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3057b866fd089c44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而评价MAXINF的标准是：

1、扩散效果：以求解的节点集合进行影响力扩散，扩散范围能够达到最大化。

2、节点搜索效率：在最短的时间内，搜索到k个初始节点。 

#### 4.2.2 MINTSS

即初始节点集最小化问题，是在已知扩散范围条件下，求解节点个数最小的初始节点集合。

#### 4.2.3 MINTIME

即扩散时间最小化问题，是在已知扩散范围条件下，求解扩散时间最小的初始节点集合。

## 5 序列模型

### 5.1 神经网路基础 

大部分只是点在吴恩达机器学习笔记中有提到，这里只做机器学习笔记中未提到的知识点梳理。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-adff4456dacfba45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上述是对机器学习的一般流程做的梳理，需要注意验证集的作用，即训练好的模型需要通过验证集的验证，如果效果达不到标准，需要重新去设置超参进行再次训练。

在优化参数的过程不仅可以使用全局的梯度下降（批量梯度下降），也可以使用随机梯度下降法。做法是损失函数一开始不是全局的，而是使用一个一个的点作为目标，虽然计算快了很多，但是很不稳定。

当然还有一种折中的方法叫做小批量梯度下降，式随机抽取m个点作损失函数，然后再用它进行参数$\theta$更新。

理解一下判别式模型和生成式模型的区别，判别式模型就是类似用作分类的模型，给定一个输入x，可以得到一个+1/0/-1的分类结果；生成式模型就比如那些线性模型一样，通过不同的输入x，可以映射得到一个输出y。

反向传播的目的是为了帮助梯度下降去求解偏导数，其实可以简单的理解求神经网络中每个中间参数$\theta$的偏导数的具体步骤，即使用链式法则进行求导。![image.png](https://upload-images.jianshu.io/upload_images/7810235-87381449623fc189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)上图为一个直观的理解，其实将$f_{\theta_j}$比作$ax^2$的话就比较好理解了。这是对标量的理解，如果换成向量或者矩阵的话，我们应该如何操作。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-b175a4bdeb0cfafb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其实就是简单的对向量/矩阵中的每一个元素求偏导再组合到一起。在进行正向传播的最后带入激活函数之中，常见的激活函数有sigmod、tanh、ReLU等。在通过激活函数之后，需要对结果进行一次softmax，在之前的笔记中也有讲过，就是将输出值，转换为概率值，概率相加为1。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ff0cfaee2c44333b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

需要注意的是x，y均为向量，他们求导的话会变成一个矩阵，如上式中softmax求偏导的时候，当向量的下标相同的时候，偏导为$y_i(1-y_i)$，当下标不同的时候，偏导为$-y_iy_j$。再多加一步可能普通的神经网络并没有涉及到，叫做embedding lookup，可以将int的值转换为embedding（向量）。这一块的理解还不怎么深刻，没办法给出较好的解释。

我们可以以一个例子来加强理解，网络如何使用到语言学习中。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-22f6886741bcfc21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面是例子的形式话，把模型的训练集转换为bigram的形式，并将字符以数字的形式表示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-dbe930159c462669.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上式为前向传播的过程（图中的b修改为1），我们的目标是让出现a之后，得到b的概率最大化。具体步骤是：对a进行初始化，使之变为词向量h1，再通过线性变化y=wx+b得到h2向量，引入激活函数对结果进行“压缩”后得到的是h3，再进行一次线性变化得到h4，之后通过softmax得到每个输出的预测概率h5，并通过cross-entropy（交差熵）得到损失函数。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-13ee3b4df56e6ab4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在完成前向传播的过程的同时，我们同样需要通过梯度下降来完成参数的优化，上图中展示的是通过后向传播对偏导数的求解。可以知道的是我们的目标是把最后求得的损失函数对每个参数以及输入求偏导，通过链式法则，我们需要不断的如图中由上至下去一步一步求得偏导，将前向传播和后向传播都总结一遍可以如下图所示：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-581f2af68988949d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个地方的求偏导$\frac{dj}{dx}=\frac{dj}{dy}\frac{dy}{dx}$,前面的$\frac{dj}{dy}$是上一层所传过来的结果，直接引用就行了，$\frac{dy}{dx}$则是这一层需要求得的偏导数。上面的$y=\sigma(x)$说的是sigmod函数，$y=enl(x)$的意思是embedding look up，即初始化词向量，下图为网络的直观表达形式。 

![image.png](https://upload-images.jianshu.io/upload_images/7810235-ba29ff3d2f56df13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

看这种就比较有亲切感了，也更抽象，中间的h1是通过将两个词的词向量组合起来得到的。h2是通过线性变换和激活函数得到的，之后经过softmax，得到标签的概率值。

### 5.2 RNN理论

循环神经网路（RNN）不同于普通的神经网路，对中间的某一层可能出现迭代计算，计算得到的结果再丢到这一层里面继续做运算（这个时候并没有优化参数），如果把普通的神经网络理解为纵向延伸的话，那么RNN就是在横向进行延伸。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-761a1d576de2d043.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图中是计算的简单例子，初始化一些参数以后，把$h_{t-2}$带进去以后，算得$h_{t-1}$为0.462，再将结果继续带到该函数$f$里面。这是它的前向传播的过程，那么如何进行反向传播呢。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-a10b025598ab1d1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设初始输入的是$h_{t-1}$和$x_t$，我们一般常规的思路去求导就如同上面的式子的计算步骤，需要注意的点是应该初始的输入为$h_{t-1}$，所以$\frac{dh_{t-1}}{dw}$的结果为0,所以$\frac{h_t}{dw}=\frac{tanh(a_t)}{da_t}*(h_{t-1}+\frac{dh_{t-1}}{dw})=\frac{tanh(a_t)}{da_t}h_{t-1}$。这种方式的计算比较复杂，我们还有另外一种解决思路，将中间参数w标为不同的值，$h_{t-1}$输入的$f$的参数w为$w_2$，$h_{t}$输入的$f$的参数w记为$w_1$那么求w的偏导的过程就变为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-6e67d830fbdbde3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图可知，最后分别对$w_1$和$w_2$求偏导以后的结果刚好是直接对$w$求偏导的结果的两项。所以$\frac{dj}{dw}$就是不同位置的$w$求偏导再相加。同样的参数U、b也可以当做不同的参数进行计算。推算到一般式是

![image.png](https://upload-images.jianshu.io/upload_images/7810235-1480f5fa37406c95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们要算某个$w$的偏导数$\frac{dj}{dw_t}$的步骤如上图，大致也是链式法则对其求导方式，中间会有n个$\frac{dtanh(a_i)}{da_i}W$相乘。当$w$<1的时候(假设$w$是一个一维矩阵)，连乘会导致偏导数过小，从而造成梯度消失，反之，过当$w$>1的时候，会造成梯度爆炸。相对而言$\frac{dtanh(a_i)}{da_i}W$造成梯度消失的可能性不大，因为他不是一个定值，只有$a_i$膜大于2的时候才可能造成梯度消失。

梯度爆炸即当所求的梯度值过高的时候，可能在参数优化的时候变化巨大，导致之前的优化工作白费，关于梯度爆炸的解决方案是：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cd80e7207830303a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里的$\triangledown$指的是不同的参数对损失函数的偏导数，而$|\triangledown|$的意思是将所有偏导数平方求和后再开放，并设置阈值c，当高于阈值的时候，原偏导数$\triangledown$需要乘于$\frac{c}{|\triangledown|}$。

梯度消失即所求的梯度值过小（接近于0），导致参数还没更新到最佳值的时候就已经基本停止更新了，可能本身损失函数的选择不怎么好，关于如何解决梯度消失，有一种思路就是使用LSTM。

### 5.3 LSTM

LSTM（Long Short-Term Memory）长短期神经网路是对RNN的改进，可以有效的解决梯度消失的问题。他比RNN多了一个机制，叫做忘记门。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-a65a5fa8f12b5de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以如上图所知，在之前的RNN的基础多了一个参数$C_t$。可以看到不仅计算出了$h_{t}$，还计算出了$C_t$，计算的步骤主要是线性操作。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fdd8b4453a6cbb5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里的$f$就是代表的忘记程度，可以发现的是当我对$C_t$求偏导的时候，即$\frac{dC_{t+1}}{C_t}=f$，在不同的位置上$f$可以不同，所以连续多个值接近1的时候，不同于之前$\frac{dj}{dw_t}=\frac{dtanh(a_i)}{da_i}W$，$W$可能出现梯度消失，这里的$\frac{dC_{t+1}}{C_t}$保证梯度不会消失。关于如何确定$f$以及$i$，我们可以看下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-e0028ea5fb0ba659.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图所示，就是$C_t$如何进行计算的的，可以看到其计算的过程是不会离开当前的输入$x_t$和之前所计算得到的$h_{t-1}$的。当然我们这里还没有对$h_t$进行计算，这里跟RNN计算$h_t$的过程有些差别。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-2160e67a13ed7be1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里得$O_t$同样也是一个忘记门，$h_t$的结果就是由之前计算的向量$C_t$与$O_t$乘积得到的，可以看到整个过程有4组参数$W$和$b$。这里就是LSTM的核心部分了，下图开始说的是整个LSTM的运作原理。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-989f0a5ffe0eafa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

EOS值得是开头的意思，比如$p=(W|EOS)$的意思就是以单词W开头的概率，我们一开始将EOS的词向量丢到网络中，经过线性变化和softmax等操作以后，得到每个标签的概率（这里可以假定为5个标签，要使得W在开头的概率最大化），然后我们将$h_t$和$c_t$带到下一次的计算中，将$h_t$用于计算损失函数,我们在每经过一层网络都能得到一个损失函数。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0dec8feb4b78c671.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图是运用了多层的LSTM得到的效果，总的损失函数还是每个预测结果的损失函数的相加，在作反向传播的时候，需要回退到之前每个地方做计算，感觉很复杂。这里引入一个概念dropout，他属于正则化的一种方式。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-2d020353324a93a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

操作比较简单，类似于“忘掉”一些操作，将他带入之前说的两层的LSTM之中

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9756492f3484d194.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

具体的操作是在纵向添加dropout，这里还不是很了解其实际的作用，大致的意思是防止模型训练的过拟合。

### 5.4 RNN命令与代码

#### 5.4.1 Linux命令

针对自然语言的数据集，可能很多处理不需要通过代码，而直接通过命令就能完成。

wc 文件名：可以得到文档有多少行已经多少个字（token，即总过包含的字数，算重复的）

![image.png](https://upload-images.jianshu.io/upload_images/7810235-64e397d210f1725d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图所示，第一个数字代表行数，第二个数字代表字数

------------------------------------------------------------------------

cat 文件名 | tr ' ' '\n'|sort|uniq -c|sort -n

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fdae575a5633486f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

tr是用于替换；

|sort是用于排序；-n是搭配sort是的结果从小到大排序

|uniq是用于去重复的行；-c是搭配sort使用可以统计重复的次数；

|wc跟上面一样，可以看到有多少单词

上述的命令可以实现将文本中所有字（type，不算重复）找出，并讲重复的做记录，排序显示出来 

------------------------------------------------------------------------

awk '{print NF}' 文件名 |sort -n|uniq -c

![image.png](https://upload-images.jianshu.io/upload_images/7810235-621fed8e5f719c07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

awk命令很复杂，相当于输入指令，可以实现一些功能，这里的意思是可以统计一行字的个数，上面可以看到一行只有一个字的行数有137行（一行为一句话）

------



#### 5.4.2 代码

代码目前是案例，在Linux上面可以跑（Python2.7在Windows环境下不怎么兼容TensorFlow）

主要的文件介绍：

/RNNLM/data：中间存放的数据集

/RNNLM/py:大致的结构如下图

![image.png](https://upload-images.jianshu.io/upload_images/7810235-925ccb6ee02082e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

下面我们会一一介绍

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f1c4fd2af30e633c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

data_util的作用主要是对数据集进行预处理作用，讲句首等信息加到数据集中，另外还可以对单词进行数字替换，替换的数字这里是依据词频的排序定的。当运行这个文件的时候，会在/model/data_cache这个目录中放如预处理后的文件（vocab，每行一个词），以及单词替换成数字以后的文件（train.id）。具体的代码讲解可以查看代码中的注释。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-a068f85e85717eb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

主要是用于运行各个代码中的方法，主要是看main函数。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-b205d7c529ec4694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为训练方法的主要思想，opoch的意思是训练了多少次，当10次perplexity都没有降低的时候，默认为模型已达到最优，如果中间perplexity有升高的话，就将学习率减半。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0fe4e29d90393ef1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

之前在神经网络基础曾经提到过网络中数据的一个基本流程，当时输入是一个词的向量表示，我们这里可以扩展为多个个词的向量表示，比如上图中就是输入的ab两个次，那么之前的向量运算就变成了矩阵运算。下面我们以一个例子说明：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fa77930b60b7f1ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，输入为两句话，可能出现单词数不匹配的情况，那么我们可以通过添加填充词pad来解决这个问题。但这样会引起损失函数的增大。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-672f35de3b2b32f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

操作就是将损失函数的矩阵点乘一个与它结构相同的矩阵（mask），其中为pad的地方置0，这样就可以消除pad对损失函数的影响。但是如果存在极少数的句子，单词数过多的话，就会出现利用率较低的情况（为了补齐，加入太多的pad）。解决方式如下：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4a5cf66d447edf0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

讲长度较近的进行分组，这样就大大的减少了浪费，如何分组需要用到的就是之前/py目录下的buckets文件。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-97f5d481b74c0d0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

best_buckets.py文件就就实现了我们上述讲到的一些功能，一般是进行平均分组。在run.py文件中177左右有写是如何对句子进行分组的，结果的形式为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-b361e006c19b41ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

分别对句子长度小于2的，以及小于4的进行分组。每个数字其实都是代表长度为该数字的不同句子。上述有讲到计算梯度下降的方法是采用的是mini-batch，所以如何选取数据集也要考虑：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-49842c7b26f4f5ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在数据处理的时候要把GO和EOS去掉，并在尾部补上pad，这里的weights是之前所提到的mask矩阵，用于在求损失函数的时候去掉填充词pad的影响。然后就是模型的构建了：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-de1ef46e5266f29b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

主要是分为这个三个方法，下面会对这三个方法做详细的解释。第一个init函数顾名思义就是初始化函数，给定初始的参数用于构建模型了；第二个函数是用于分配数据集中，那些作为训练集；step函数可以实现前向后向传播和参数的更新。下面我们先介绍init函数的内容。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-03251ccbb8b63fd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上述过程就是单词初始化为向量的过程，通过输入每个词特有的一个序号，然后在placehold里面去找到相应的向量表示形式，来将每一句话转换为向量。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-1a5f240c8730b33d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这一部分是创造LSTM结构的部分，采用的是TensorFlow中间关于LSTM构建方式，可以在代码中查看详细的步骤。最后就是输出部分了。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f8acc731d4af2852.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

跟输入的方式比较相似，先输出的词的序号，之后对应着placehold，转换为向量。 

### 5.5 seq2seq

#### 5.5.1 基本概念

seq2seq的核心可以理解为根据一个输入的序列得到一个输出序列的概率，具体的做法是：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-63ddf60bb985f7e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中N为输出序列的长度，M为输出序列的长度，所以输出序列中的某一个词$e_i$的概率是跟输出序列$e_i$的之前词以及输入序列有关，不同于之前学的LSTM，只跟一个序列有关，这里多加了一个输入序列。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fa7933b299ebd0ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

以上可以发现很多NLP任务都可以转换为使用seq2seq模型去实现，做法很多时候就是改变输入和输出的一些序列。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-60e2f6f221598447.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从上图我们可以看到，seq2seq实际上是两个两层的LSTM相互作用的结果，左边这个就是由输入序列F生成的h、c结果（可以理解用序列模型的方式表示出来的输入序列），然后将结果作为右边这个两层LSTM的输入。我们把左边的结构称为encoder，右边的结构称为decoder。两者的结构还是有一些不同的。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-fa077fe44b5159d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

首先不同当然是encoder和decoder中建所带的参数，其次就是在encoder中，我们是不需要预测什么东西的，这样说还是比较笼统，更为具体的步骤就是，在之前所讲的LSTM中，每一次循环的输出都可以作为下一次循环的输入（比如这张图中的$e_i$），不管是训练的时候还是测试的时候都是这样，但encoder中，并不需要，它不用求出损失函数之类的东西，只管最后输出一个序列的向量表示形式（视频中是这样说的，但参数怎么训练在这里还不理解）。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cd03784d9fbc3650.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为seq2seq的主要参数，**超参数**有层数（LSTM的层数）、hidden size（向量的维数）、$V_F$和$V_E$（F和E数据集大小）；**encoder**有input(输入，为矩阵的形式)、LSTM中间的参数（之前的知识可以知道中间的参数主要有f、i、o、c，这里面每个参数实际上有两个权值矩阵和一个偏执向量，所以一层LSTM包含$8d^2+4d$）；**decoder**有input（输入，为矩阵的形式）、LSTM中间的参数和output（里面有词向量组成矩阵和偏置向量），**计算量主要集中在这里**。 

#### 5.5.2 Beam Search

seq2seq实际是求得每种序列出现的条件概率，我们实际想要的结果是一个输出的序列，那么我们就需要让这个我们想要输出的序列概率是最大的。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-8b64b175b34c69f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们发现想要计算出最大概率的序列，其实可以类比成上图这样的求路径的方式，最最简单的方式是通过贪心的策略来求解，但局部最优不一定能达到全局最优，所以这里采用了动态规划的思想，直接看上图就行了。他的时间复杂度是$O(V^2N)$，空间复杂度是$O(VN)$。可以看到动态规划的计算代价还是很大的，这里就提出了Beam Search来解决这个问题。下面以一个列子作为讲解：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-02357a7062c09435.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们还是对这种类似的例子求概率，其实做法跟之前的动态规划的思想还是有点像的，我们定义一个Beam Search的大小为2，所以我们每一步都虽然都按照上面的方式进行计算，但只取前Beam Search个概率最高的序列，然后之后每次序列都取前2的概率最大的序列。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-d6ec7ea72734c598.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图为Beam Search的算法思路，可以$s(b,n)$指的是表中的序列出现的概率，$h(b,n)$为表中出现的序列。他的时间复杂度为$O(Blog(V)VN)$，空间复杂度为$O(BN)$。下面我们将Beam Search带入seq2seq中去讲讲。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-50611bc9814e199a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图可以只是由encoder传过来的h、c两个参数，第一步传go得到a、b两个结果，然后第二步以一个矩阵的形式再将a、b传如的时候得到ab、ac两个结果，这个时候如图中红色标注的地方所示，因为最后结果都来自a，所以在传入下一次LSTM计算的参数h、c的时候应该将b给“去掉”。具体的做法是第二步的时候我们输入的矩阵是$[[0.3,0.4][1,-1]]$，但在计算出Beam Search以后，我们可以在计算传给下一次LSTM的结果的时候，将输入改成$[[0.3,0.4][0.3,0.4]]$，消除b的影响。

#### 5.5.3 Attention

![image.png](https://upload-images.jianshu.io/upload_images/7810235-1bfecccd312a21ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

虽然seq2seq貌似解决了传统的n-gram模型只能决定与有限长度的序列的问题，但实际情况中在序列过长的时候，往往因为LSTM的特性，已经差不多“忘了”原来的信息，比如机器翻译中，在you翻译成你需要经过五步才能达到，那么如何解决这个问题呢，做法是在encoder的上方做一条连接，直接连接到decader的那一方，如何确定我们的连接到哪个位置就是通过Attention完成的。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-18e608065e225484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)











