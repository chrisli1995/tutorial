# 自然语言处理

笔记中包含了NLP的学习以及所设计到语义关系的一些技术，比如社交网络等。**默认有一定的基础**

## 1 概论

### 1.1 什么是自然语言处理（NLP）

使用计算机处理文本与声音

本质上说的是将文本进行转换，可以理解为：

-  **文本->表示**：比如情感分析就是将句子转换为-1/0/+1等不同的值，而句法分析则是将句子转换为语法树（AST）
-  **表示->文本**：比如可以通过主题词生成诗歌
-  **文本->文本**：比如机器翻译


### 1.2 常见应用

应用范围非常广泛，就比如在平时用的输入法上也有很多的利用：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-315dbc673bade36c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如在机器翻译中的应用：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-408f8938caf4e739.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如搜索引擎中的自动化处理：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-36122d4c00c61483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如我们看文章中时候出现的推荐功能：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0dce5221619b3851.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如我们之前也见过的聊天机器人：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-b8936bf6efbf8792.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如英文写作助手，这个其实不怎么常见：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-aa6cfa3729a6c71a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当讲完这些基本的应用背景以后，也有一些比较新颖的应用背景，比如：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-8108855b103c4d5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这一类设计的技术可能就不仅仅包括NLP方面，可以看到通过Twitter检测重要事事故，同样需要用到社交网络等知识。也有与图像识别一起使用的一些应用点，比如：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-cb497517b5c643d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再比如，可能运用的方面比较新颖，技术方面不会太过复杂：

![1533027739842](C:\Users\59845\AppData\Local\Temp\1533027739842.png)

再比如一些谷歌的自动回复功能：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0abdb25b6c4a1ff2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于上面这些应用，我们所涉及到的技术可以总结为如下图：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4de148edf5092229.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

社交网络和可信度分析不完全属于NLP的范畴，但有用到。其他的是需要了解的技术。后面会有讲解。

### 1.3 挑战

语言的挑战主要还是如何准确的判断出每个词在语句中的真实含义，比如需要看上下文的语境。对于自然语言处理，他处理的层次如图。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-220e7ab9ffcb49ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

大致总结为我们需要使用词法和语法来分析语义和语用，语用的概念为句与句之间的关系。

其次，对于领域隔离也是一个难点所在，不同领域的不同情况，机器的理解还是完全跟不上的，跨过任何一个领域都比较困难，一种新颖的观点是语言自己就是最好的表示。

再然后就是数据集的搜索难度较大，比如少数民族的语言或者被和谐的语言。

最后很多情况下无法评判结果的好坏，比如机器翻译，一句话有多种意思都是对的，不能说哪个翻译是对的哪个是错的。  

有一些常用的工具如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-f411d818dee8c1e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2 语言模型

**语言模型**抽象的理解就是为说的每一句话做一个评分，可以形式化的理解为score(文本)-->评分（score）。以此还可以扩展出**话题模型**，其形式为score（话题|文本）-->评分（score），比如可以定义score（NLP|什么是语言模型）-->0.8，而score（三国|什么是语言模型）-->0.05，这种模式就是话题模型。

而在NLP中的语言模型，所说的是概率语言模型。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9e9d89d55e1a2468.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，式子反应一句话出现的概率，所有话出现的概率总和为1。

### 2.1 N-gram模型

目的是求一句话出现的概率，采用了链式规则，其实比较简单，大致的思想就是

![image.png](https://upload-images.jianshu.io/upload_images/7810235-9e04af9049a7339f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是这样计算的参数比较多，所以引入了马尔科夫假设，即无记忆性（未来的事件，只取决于有限的历史）的概念。 

![image.png](https://upload-images.jianshu.io/upload_images/7810235-59f080efb8941307.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

N-gram模型设定的是一个词出现的概率，之前其之前有限个词出现的概率有关，这样就大大的减小了计算的难度，但RNN（循环神经网络）很好的解决了计算难度大的问题，所以又可以直接采用链式规则求出一句话出现的概率。常见的N-gram模型有：

**Bi-Gram**:P(T)=p(w1|begin)*p(w2|w1)*p(w3|w2)***p(wn|wn-1)

**Tri-Gram**:P(T)=p(w1|begin1,begin2)*p(w2|w1,begin1)*p(w3|w2w1)***p(wn|wn-1,wn-2)

### 2.2 语言模型的评价标准

评标标准在任何模型中都是非常的重要，如何较好的评价出模型的评分或者预测的准确率至关重要，语言模型中大致还是使用预测测试集的准确程度来评价。具体的步骤可以如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-8a636412f181adde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们的目的在于将得到的perplexity的值最小化，**$q(w_i)$的意思是，模型计算出来的$w_i$在前后关系中出现的概率**。那么perplexity到底是什么意思呢，其实可以表示为测试集的等效状态的数目，再通俗一点可以理解为我需要通过多少词才能预测准确测试集。他的具体数学逻辑可以这么理解：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4c719e4355330c05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中的N代表的是测试集中所包含的单词数，V则代表测试集中不算重复的单词数，经过转换可以得道最后的公式，其中$\hat{p}(v_i)$的意思是$v_i$在测试集中出现的概率。最简单的思想就是我们的目标是最大化这里面出现的词的概率，即$q(w_i)$增大的时候perplexity是减小的，可能这里还是不是很理解，但至少要理解上述所说的话。

### 2.3 N-gram模型遇到的难点

这里我们引入一个新的概念，OOV（out of vocabulary），顾名思义就是说没有的单词，即我们的模型如何预测训练集中没有的模型，这是语言模型中的一个难点。另外，当实际情况中，出现了没有出现的单词组合我们怎么办，这也是一个难点。下面我们可以举个例子来说明下：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-36566c5a9b356a06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当遇到OOV的情况的时候，我们改如何处理呢，$P(w_i|w_{i-1},w_{i-2})$的意思是当出现$w_{i-1}$和$w_{i-2}$的时候，下一词是$w_i$的可能性。对于第一种情况，测试集出现了训练集不存在的词，比如王者农药，做法是将训练集中出现词频最小的词改为“UNK”,这样王者农药就属于这里的UNK。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-5d1e9fa97c838bad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到修改后，出现王者农药的概率由原来的0到现在的$\frac{1}{3}$。对于解决我们没有出现的序列，其实有很多的方法，大致可以分为下图中的三类。![image.png](https://upload-images.jianshu.io/upload_images/7810235-b7f48b4561148e9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**+1平滑**相当于在概率的基础上加上一个值，使得概率不会变为0，但语言模型一般不使用。

**Back-off回退法**说的是当没出现出现的序列的情况下，我们可以简化序列，比如将Trigram的情况会出现有的概率为0的情况，则使用Brigram，如果还有概率为0的情况可以使用Unigram。

**Interpolate插值法**所用的方法是将Trigram、Brigram、Unigram一定比例混合使用，这样的话即使Trigram出现没有的序列而导致概率为0，其他几种情况的概率依旧可以使得最后的概率不为0,参数的选择是通过训练集得出，并且可以依据不同的词来调节参数。

**Absolute Discount**字面上理解就是绝对折扣的意思，大致的思路就是设置一个阈值D，当没达到这个阈值D的时候，改为使用Unigram。 ![image.png](https://upload-images.jianshu.io/upload_images/7810235-f384c5d1d372d349.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)可以更抽象的理解是，当一个赚的钱不足D的时候，我只能去依靠向别人借钱来维持，这里的别人就是后面的$P_{abs}=(w_i|w^{i-1}_{i-n+2})$,即Brigram。

**Kneser-Ney Smoothing**这种方法是对**Absolute Discount**的改进，对于**Absolute Discount**可能会出现在使用Unigram的时候频率比价高但与实际效果并不符合，例如（刘强东 奶茶）这个组合出现的比（王思聪 张予馨）和（王思聪 豆得儿）等一些词加起来还多，但当你要预测P（*，网红）的时候，并不希望出现的是刘强东的概率更高，所以我们希望“交际广泛”的王思聪出现的更多。![image.png](https://upload-images.jianshu.io/upload_images/7810235-974bbcac5b4f75c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)上图中就说明主要改进的是$P_{KN}$，$N_{1+}(x)$的概念是所有出现的词组的**个数**（不是次数）。 

**Modified Kneser-Ney Smoothing**则是对**Kneser-Ney Smoothing**又一改进，设置了不同的D值。![image.png](https://upload-images.jianshu.io/upload_images/7810235-454db205c3396495.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)这个地方的理解还不是很深刻。。。

下面是对所有方法的一个总结：![image.png](https://upload-images.jianshu.io/upload_images/7810235-fdb61ef7bc789efa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 2.4 代码实现

对于环境可以是anaconda的科学python库，采用nltk包对英文文本进行分词。主要的示列存在GitHub上，地址为：https://github.com/chrisli1995/xing_nlp。

## 3 word2vec原理

### 3.1 为什么是word2vec？

在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder（独热码）。 如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/7810235-0b27b582c6d5c8fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot  Encoder有以下问题。首先，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 

**Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。** 

word2vec模型一般采用三层的神经网络结构，为输入层、隐层和输出层，如图所示：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-855f38ac630ff1e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 3.2 基本知识

#### 3.2.1 哈夫曼树	

#### 3.2.2 逻辑回归

### 3.3 CBOW和skip-gram



## 4 社会网络

### 4.1 基本概念

社会网络中，个体之间的互动和交流，包括信息的推荐分享等，使得信息在网络中得到扩散。社会网络的结构可用**有向图**G=(V,E)表示，其中节点集合V中的元素代表社会网络中的社会个体，边集E中的元素代表社会个体间的关系。有向图中每个节点初始时均处于“未激活”状态，为“未激活节点”，如果网络中的某个个体接受某个观点，或购买了某个产品，则称该个体对应的节点处于“激活状态”，转变为“激活节点”。

下面会介绍一些社会网络的基本概念：

**激活**： 节点u 处于激活状态时，会对其邻居节点中处于未激活状态的节点v 产生影响， 当节点v 受节点u 的影响转变为激活节点时， 称节点u将节点v “激活” 。 

**被激活**： 根据“激活”的描述， 节点v 受节点u 的影响作用“被激活”。 

**扩散过程**： 指网络中未激活节点受激活节点的影响， 转变成激活状态的过程， 即下图中，由t时刻向t1时刻转变的过程。其中，浅色节点为激活节点（如节点2）， 深色节点为未激活节点（如节点 10）。t到t1时刻，节点1受节点2和4的共同影响作用被激活。 在扩散过程中，节点只能由未激活状态转变成激活状态，且当节点 1 被激活后，它将一直保持激活状态，而且会再去激活它自己的未激活邻节点7和9。重复这一过程，直至网络中所有的节点都被激活，或节点状态均不再发生变化为止。 

![image.png](https://upload-images.jianshu.io/upload_images/7810235-4d08e9075b3732d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**影响力**： 社会群体中，当社会个体表现出某个行为后，可能会改变群体中其他个体的思想、情感或行为，这种影响其他个体的能力称为该个体在该群体中的影响力。 

### 4.2 影响力扩散问题

#### 4.2.1 MAXINF

即影响最大化问题，探索一些初始的有影响力的节点，使得最后从节点扩散开的节点达到最大化。

问题的形式化描述为：

![image.png](https://upload-images.jianshu.io/upload_images/7810235-3057b866fd089c44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而评价MAXINF的标准是：

1、扩散效果：以求解的节点集合进行影响力扩散，扩散范围能够达到最大化。

2、节点搜索效率：在最短的时间内，搜索到k个初始节点。 

#### 4.2.2 MINTSS

即初始节点集最小化问题，是在已知扩散范围条件下，求解节点个数最小的初始节点集合。

#### 4.2.3 MINTIME

即扩散时间最小化问题，是在已知扩散范围条件下，求解扩散时间最小的初始节点集合。

## 5 序列模型

### 5.1 基础 

